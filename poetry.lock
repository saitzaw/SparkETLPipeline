[[package]]
name = "py4j"
version = "0.10.9.7"
description = "Enables Python programs to dynamically access arbitrary Java objects"
category = "main"
optional = false
python-versions = "*"

[[package]]
name = "pyspark"
version = "3.5.0"
description = "Apache Spark Python API"
category = "main"
optional = false
python-versions = ">=3.8"

[package.dependencies]
py4j = "0.10.9.7"

[package.extras]
connect = ["googleapis-common-protos (>=1.56.4)", "grpcio-status (>=1.56.0)", "grpcio (>=1.56.0)", "numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=4.0.0)"]
ml = ["numpy (>=1.15)"]
mllib = ["numpy (>=1.15)"]
pandas_on_spark = ["numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=4.0.0)"]
sql = ["numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=4.0.0)"]

[metadata]
lock-version = "1.1"
python-versions = "^3.9"
content-hash = "1f35ca8c522448d8e7cb6e81904129e1018a220325de1b2f9c5a685d91b4632a"

[metadata.files]
py4j = []
pyspark = []
